{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NOTE**\n",
    "\n",
    "ONLY NEEDS TO RUN ONCE! Make sure you absolutely need to run this before running it. If datafiles already exist, then you probably don't need to rerun this.\n",
    "\n",
    "Do NOT run without unzipping `T_ONTIME_REPORTING_unprocessedData.zip` file first and move data files to main data directory, otherwise this code will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir='/Users/madelinefrank/Documents/KSU/Course_Work/5_Summer2023/CS_7265/Project/data'\n",
    "yrs=['2017','2018','2019']\n",
    "mos=['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "fn_fmt='T_ONTIME_REPORTING_{yr}{mo}.csv'\n",
    "\n",
    "cols=['YEAR','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','FL_DATE','OP_UNIQUE_CARRIER',\n",
    "      'ORIGIN','ORIGIN_STATE_ABR',\n",
    "      'DEST',    'DEST_STATE_ABR',\n",
    "      'CRS_DEP_TIME','DEP_TIME','DEP_DELAY','DEP_DELAY_NEW','DEP_DEL15','DEP_DELAY_GROUP',\n",
    "      'CRS_ARR_TIME','ARR_TIME','ARR_DELAY','ARR_DELAY_NEW','ARR_DEL15','ARR_DELAY_GROUP',\n",
    "      'CANCELLED','DIVERTED','CRS_ELAPSED_TIME','ACTUAL_ELAPSED_TIME',\n",
    "      'DISTANCE','DISTANCE_GROUP',\n",
    "      'CARRIER_DELAY', 'WEATHER_DELAY','NAS_DELAY','SECURITY_DELAY','LATE_AIRCRAFT_DELAY']\n",
    "renamecols = {'YEAR':'YR','MONTH':'MON','DAY_OF_MONTH':'DAYOFMONTH','DAY_OF_WEEK':'DAYOFWEEK',\n",
    "            'OP_UNIQUE_CARRIER':'CARRIER',\n",
    "           'ORIGIN':'ORIG','ORIGIN_STATE_ABR':'ORIG_ST','DEST_STATE_ABR':'DEST_ST',\n",
    "           'CRS_DEP_TIME':'CRS_T_DEP','DEP_TIME':'ACT_T_DEP',\n",
    "           'CRS_ARR_TIME':'CRS_T_ARR','ARR_TIME':'ACT_T_ARR',\n",
    "           'DEP_DELAY_NEW':'DEP_DELAY0','DEP_DEL15':'DEP_DEL15_TF','DEP_DELAY_GROUP':'DEP_DEL_GRP',\n",
    "           'ARR_DELAY_NEW':'ARR_DELAY0','ARR_DEL15':'ARR_DEL15_TF','ARR_DELAY_GROUP':'ARR_DEL_GRP',\n",
    "           'CRS_ELAPSED_TIME':'CRS_T_ELAPSED','ACTUAL_ELAPSED_TIME':'ACT_T_ELAPSED',\n",
    "           'DISTANCE':'DIST','DISTANCE_GROUP':'DIST_GRP',\n",
    "           'WEATHER_DELAY':'WX_DELAY','SECURITY_DELAY':'SEC_DELAY'}\n",
    "airports=['ATL', 'ORD', 'DEN', 'LAX', 'DFW', 'PHX', 'SFO', 'LAS', 'SEA', 'MSP',\n",
    "          'MCO', 'DTW', 'BOS', 'SLC', 'CLT', 'BWI', 'EWR', 'JFK', 'FLL', 'MDW',\n",
    "          'IAH', 'SAN', 'LGA', 'PHL', 'TPA', 'DCA', 'MIA', 'DAL', 'PDX', 'BNA',\n",
    "          'STL', 'HOU', 'AUS', 'OAK', 'SJC', 'MSY', 'SMF', 'MCI', 'SNA', 'IAD',\n",
    "          'RDU', 'SAT', 'RSW', 'MKE', 'PIT', 'CLE', 'IND', 'BUR', 'PBI', 'SJU',\n",
    "          'CMH', 'BDL', 'ONT', 'ABQ', 'CVG', 'OMA', 'ANC', 'JAX', 'BUF', 'LGB',\n",
    "          'BOI', 'RNO', 'TUS', 'OKC', 'HNL', 'MEM', 'PVD', 'CHS', 'GEG', 'TUL',\n",
    "          'RIC', 'ELP', 'ORF', 'ALB', 'GRR', 'COS', 'SDF', 'PSP', 'OGG', 'BHM',\n",
    "          'FAT', 'MSN', 'DSM', 'ICT', 'ROC', 'MYR', 'SAV', 'LIT', 'SYR', 'MHT',\n",
    "          'ISP', 'DAY', 'ASE', 'CID', 'GSP']\n",
    "oconus=['BET','BRW','CDV','DLG','ADQ','FAI','GST','JNU','AKN','KTN','ANC','OME','OTZ','PSG','SCC','SIT','WRG','YAK','ADK','PPG','GUM','KOA','LIH','HNL','OGG','ITO']\n",
    "airlines=['AA','B6','DL','F9','NK','UA','WN']\n",
    "cols_model = ['DAYOFWEEK','DAYOFYEAR','CARRIER','ORIG','ORIG_ST','DEST',\n",
    "              'DEST_ST','CRS_HR_DEP','CRS_HR_ARR','DIST_GRP','ARR_DEL15_TF']\n",
    "\n",
    "#Settings\n",
    "# Determine encoding method for categorical data\n",
    "#isOneHot = True #For  True = use 1 hot encoding, False = use regular numeric encoding\n",
    "cols2enc=['CARRIER','ORIG','ORIG_ST','DEST','DEST_ST']\n",
    "enc_groups={'CARRIER':['CARRIER'], 'AIRPORT': ['ORIG','DEST'], 'STATE': ['ORIG_ST','DEST_ST']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of files to be preprocessed\n",
    "def getNumFNs():\n",
    "    num_fns = 0\n",
    "    for yr in yrs:\n",
    "        fns = glob.glob(os.path.join(datadir,fn_fmt.format(yr=yr,mo='*')))\n",
    "        num_fns+=len(fns)\n",
    "    return num_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and reformat data (no encoding)\n",
    "def fmt_clean_data(df):\n",
    "    try:\n",
    "        # airports=pd.read_csv(os.path.join(datadir,'airports_delay_data_xs.csv'))\n",
    "        df.FL_DATE=pd.to_datetime(df.FL_DATE,utc=False,dayfirst=False,yearfirst=False,format='%m/%d/%y %H:%M')\n",
    "        tmp=['DIVERTED','CANCELLED','DEP_DEL15','ARR_DEL15']\n",
    "        df[tmp]=df[tmp].astype('bool')\n",
    "        # Remove diverted or cancelled flights\n",
    "        df=df.loc[df['CANCELLED']==False]\n",
    "        df=df.loc[df['DIVERTED'] ==False]\n",
    "        df.drop(columns=['DIVERTED','CANCELLED'],inplace=True)\n",
    "        df.rename(columns=renamecols,inplace=True)\n",
    "        df=df.loc[df['CARRIER'].isin(airlines)]\n",
    "        df=df.loc[~df['ORIG'].isin(oconus) & ~df['DEST'].isin(oconus)]\n",
    "        df=df.loc[df['ORIG'].isin(airports) & df['DEST'].isin(airports)]\n",
    "        df.insert(4,'DAYOFYEAR',df.FL_DATE.apply(lambda x: x.dayofyear))\n",
    "        df.drop(columns=['FL_DATE'],inplace=True)\n",
    "        df.dropna(how='any',subset=['ACT_T_ARR'],inplace=True)\n",
    "        df['CRS_T_DEP']=df['CRS_T_DEP'].apply(lambda x: '{0:0>4}'.format(x))\n",
    "        df['ACT_T_DEP']=df['ACT_T_DEP'].astype('int').apply(lambda x: '{0:0>4}'.format(x))\n",
    "        df['CRS_T_ARR']=df['CRS_T_ARR'].apply(lambda x: '{0:0>4}'.format(x))\n",
    "        df['ACT_T_ARR']=df['ACT_T_ARR'].astype('int').apply(lambda x: '{0:0>4}'.format(x))\n",
    "        df.insert(11,'CRS_HR_DEP',df.CRS_T_DEP.apply(lambda x: x[:2]))\n",
    "        df.insert(18,'CRS_HR_ARR',df.CRS_T_ARR.apply(lambda x: x[:2]))\n",
    "        df[['CRS_HR_DEP','CRS_HR_ARR']]=df[['CRS_HR_DEP','CRS_HR_ARR']].astype('int')\n",
    "\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        df=df[cols_model]\n",
    "\n",
    "    except:\n",
    "        # df.dropna(how='any',subset=['ACT_T_ARR'],inplace=True)\n",
    "        pdb.set_trace()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filename for model data\n",
    "def getModelFN(fn,yr):\n",
    "    tmp = os.path.basename(fn).split(yr)\n",
    "    tmp = tmp[0]+'Model'+yr+tmp[1]\n",
    "    # Remove file if it already exists\n",
    "    if os.path.exists(os.path.join(datadir,tmp)):\n",
    "        os.remove(os.path.join(datadir,tmp))\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save dictionaries for categorical feature ordinal encoding\n",
    "def encode_cat_data(dat, grp_name):\n",
    "    oe = OrdinalEncoder()\n",
    "    #oe_nums=oe.fit_transform(dat['ORIG'].array.reshape((-1,1)))\n",
    "    oe_nums=oe.fit_transform(np.asarray(dat).reshape((-1,1)))\n",
    "    oe_lbls=oe.inverse_transform(oe_nums)\n",
    "    oenc2lbls=dict(zip(oe_nums.squeeze(),oe_lbls.squeeze()))\n",
    "    # lbls2oenc=dict(zip(oe_lbls.squeeze(),oe_nums.squeeze()))\n",
    "    w = csv.writer(open(os.path.join(datadir,'oenc2lbls_'+grp_name+'.csv'),'w')) # save for future use\n",
    "    for oenc, lbl in oenc2lbls.items():\n",
    "        w.writerow([oenc,lbl])\n",
    "    # x = csv.writer(open(os.path.join(datadir,'lbls2oenc_'+grp_name+'.csv'),'w')) # save for future use\n",
    "    # for lbl1, oenc1 in lbls2oenc.items():\n",
    "    #     x.writerow([lbl1,oenc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together specified categorical features (see \"Constants\" section above)\n",
    "# to get all possible values\n",
    "def groupEncFeats(enc_dict):\n",
    "    for grp_name,grp in enc_groups.items():\n",
    "        for g in grp:\n",
    "            a=enc_dict.pop(g)\n",
    "            if (grp_name in enc_groups):\n",
    "                enc_dict.update({grp_name: []})\n",
    "            b=enc_dict.pop(grp_name)\n",
    "            a.extend(b)\n",
    "            enc_dict.update({grp_name: pd.unique(a)})\n",
    "        encode_cat_data(enc_dict[grp_name],grp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# def mk_model_data(df):\n",
    "#     # Init label encoder for categorical data\n",
    "#     oe = OrdinalEncoder()\n",
    "#     oe_nums=oe.fit_transform(df['ORIG'].array.reshape((-1,1)))\n",
    "#     oe_lbls=oe.inverse_transform(oe_nums)\n",
    "#     print(df.head)\n",
    "#     df['ORIG']=pd.Series(oe_nums.squeeze(),name='ORIG')\n",
    "#     print(df.head)\n",
    "\n",
    "#     lbls_orig_arpt = oe.fit_transform(df['ORIG'])\n",
    "# blah = np.reshape(['ATL','ORD','LAX','ATL','MCO','JFK','JFK','09B','DEN'],(-1, 1))\n",
    "# oe = OrdinalEncoder(dtype=np.int64)\n",
    "# oe_nums=oe.fit_transform(blah)\n",
    "# oe_lbls=oe.inverse_transform(oe_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/36\n",
      "  2/36\n",
      "  3/36\n",
      "  4/36\n",
      "  5/36\n",
      "  6/36\n",
      "  7/36\n",
      "  8/36\n",
      "  9/36\n",
      " 10/36\n",
      " 11/36\n",
      " 12/36\n",
      " 13/36\n",
      " 14/36\n",
      " 15/36\n",
      " 16/36\n",
      " 17/36\n",
      " 18/36\n",
      " 19/36\n",
      " 20/36\n",
      " 21/36\n",
      " 22/36\n",
      " 23/36\n",
      " 24/36\n",
      " 25/36\n",
      " 26/36\n",
      " 27/36\n",
      " 28/36\n",
      " 29/36\n",
      " 30/36\n",
      " 31/36\n",
      " 32/36\n",
      " 33/36\n",
      " 34/36\n",
      " 35/36\n",
      " 36/36\n"
     ]
    }
   ],
   "source": [
    "# Get total number of files that will be preprocessed\n",
    "num_fns=getNumFNs()\n",
    "\n",
    "# Initiate encoding dictionary (will be saved for future use)\n",
    "enc_dict=dict()\n",
    "\n",
    "count=0\n",
    "for yr in yrs:\n",
    "    fns = glob.glob(os.path.join(datadir,fn_fmt.format(yr=yr,mo='*')))\n",
    "    for fn in fns:\n",
    "        DF=pd.read_csv(fn,usecols=cols,index_col=False)\n",
    "        DF=fmt_clean_data(df=DF)\n",
    "        # Write cleaned data to new file\n",
    "        fn_new=getModelFN(fn,yr)\n",
    "        DF.to_csv(os.path.join(datadir,fn_new),header=True,index=False)\n",
    "        # Get all possible values for categorical features (see \"Constants\" section above)\n",
    "        for col in cols2enc:\n",
    "            if (col not in enc_dict.keys()):\n",
    "                enc_dict.setdefault(col,[])\n",
    "            enc_dict[col].extend(pd.unique(DF[col]))\n",
    "            enc_dict[col]=list(set(enc_dict[col]))\n",
    "        count+=1\n",
    "        print('{c:>3d}/{n}'.format(c=count,n=num_fns))\n",
    "\n",
    "\n",
    "groupEncFeats(enc_dict)\n",
    "    \n",
    "\n",
    "# Group together specified categorical features (see \"Constants\" section above)\n",
    "# for grp_name,grp in enc_groups.items():\n",
    "#     for g in grp:\n",
    "#         a=enc_dict.pop(g)\n",
    "#         if grp_name in enc_groups:\n",
    "#             enc_dict.update({grp_name: []})\n",
    "#         b=enc_dict.pop(grp_name)\n",
    "#         a.extend(b)\n",
    "#         enc_dict.update({grp_name: pd.unique(a)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
